# ğŸ¦™ ConfiguraciÃ³n de Llama3:latest

## ğŸš€ InstalaciÃ³n del Modelo

Para usar `llama3:latest` en Roxy GPT, necesitas tenerlo instalado en Ollama. AquÃ­ te explico cÃ³mo:

### **Paso 1: Verificar Ollama**
AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose:
```bash
ollama --version
```

### **Paso 2: Instalar Llama3:latest**
```bash
ollama pull llama3:latest
```

### **Paso 3: Verificar la InstalaciÃ³n**
```bash
ollama list
```

DeberÃ­as ver `llama3:latest` en la lista de modelos disponibles.

## ğŸ“Š CaracterÃ­sticas de Llama3:latest

### **Ventajas sobre Llama2**
- âœ… **Mejor rendimiento**: Respuestas mÃ¡s precisas y coherentes
- âœ… **Mayor contexto**: Puede manejar conversaciones mÃ¡s largas
- âœ… **Mejor comprensiÃ³n**: Entiende mejor el contexto y las instrucciones
- âœ… **Respuestas mÃ¡s naturales**: Lenguaje mÃ¡s fluido y natural

### **Requisitos del Sistema**
- **RAM**: MÃ­nimo 8GB, recomendado 16GB+
- **VRAM**: Si usas GPU, mÃ­nimo 6GB
- **Almacenamiento**: ~4GB de espacio libre

## ğŸ”§ ConfiguraciÃ³n en Roxy GPT

### **Modelo por Defecto**
El modelo ya estÃ¡ configurado como `llama3:latest` por defecto en la aplicaciÃ³n.

### **Verificar en la Interfaz**
1. Abre la aplicaciÃ³n en `http://localhost:3000/`
2. En el selector de modelo, deberÃ­a aparecer `llama3:latest`
3. Si no aparece, haz clic en el selector y selecciÃ³nalo manualmente

## ğŸš¨ SoluciÃ³n de Problemas

### **Error: "Model not found"**
```bash
# Instalar el modelo
ollama pull llama3:latest

# Verificar que se instalÃ³
ollama list
```

### **Error: "Out of memory"**
- Cierra otras aplicaciones que usen mucha RAM
- Considera usar `llama3:8b` en lugar de `llama3:latest`
- Aumenta la RAM virtual si es necesario

### **Error: "Connection refused"**
- AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose
- Verifica que estÃ© en `http://127.0.0.1:11434`

## ğŸ¯ Modelos Alternativos

Si `llama3:latest` es muy pesado para tu sistema:

### **Llama3:8b** (MÃ¡s ligero)
```bash
ollama pull llama3:8b
```

### **Llama3:70b** (MÃ¡s potente)
```bash
ollama pull llama3:70b
```

### **Llama2** (Alternativa estable)
```bash
ollama pull llama2
```

## ğŸ“ˆ Rendimiento Esperado

### **Con Llama3:latest**
- **Respuestas mÃ¡s inteligentes**: Mejor comprensiÃ³n del contexto
- **Memoria mejorada**: Mejor uso del sistema de memoria de Roxy
- **Personalidad mÃ¡s consistente**: Roxy se comportarÃ¡ mÃ¡s como esperas
- **Respuestas mÃ¡s rÃ¡pidas**: Aunque el modelo es mÃ¡s grande, es mÃ¡s eficiente

### **ComparaciÃ³n de Tiempos**
- **Llama2**: ~2-3 segundos por respuesta
- **Llama3:latest**: ~3-4 segundos por respuesta (pero mejor calidad)

## ğŸ‰ Â¡Disfruta de Roxy con Llama3!

Una vez configurado, Roxy serÃ¡:
- âœ… **MÃ¡s inteligente** en sus respuestas
- âœ… **Mejor memoria** y contexto
- âœ… **MÃ¡s natural** en la conversaciÃ³n
- âœ… **Mejor integraciÃ³n** con el sistema de voz

Â¡La experiencia serÃ¡ mucho mÃ¡s inmersiva y satisfactoria! ğŸš€âœ¨ 